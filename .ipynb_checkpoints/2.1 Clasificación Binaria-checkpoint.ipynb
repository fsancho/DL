{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación binaria: opiniones sobre películas\n",
    "\n",
    "Probablemente, la clasificación binaria sea el tipo de problema de ML con mayor número de aplicaciones. En este módulo aprenderemos a clasificar opiniones de películas como \"positivas/negativas\" únicamente basándonos en el contenido textual de las mismas. Será un ejemplo de lo que se conoce actualmente como **Análisis de Sentimientos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Dataset IMDB\n",
    "\n",
    "\n",
    "Usaremos un dataset que proporciona IMDB formado por 50,000 opiniones altamente polarizadas (lo que simplifica la tarea de aprendizaje). Este conjunto está dividido en 50%/50% para entrenamiento/test, y en cada uno de ellos hay un 50% de opiniones de cada tipo.\n",
    "\n",
    "Igual que pasaba con el dataset anterior (MNIST) este dataset también viene con Keras, y además se proporciona preprocesado: por medio de un diccionario indexado, las opiniones (secuencias de palabras) se han convertido en secuencias de enteros.\n",
    "\n",
    "El código que carga el dataset (unos 80Mb de datos que serán descargado la primera vez que se ejecuta) es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como el diccionario es tremendamente grande y hay muchas palabras que se usan rara vez, nos quedaremos solo con las 10,000 palabras más frecuentes (algo que conseguimos por medio del argumento `num_words = 10000` en el proceso de carga). Además, de esta forma trabajaremos con tamaños manejables.\n",
    "\n",
    "Si quieres, puedes explorar el contenido de las variables `train_data` y `test_data`, que son listas de opiniones, donde cada opinión es una lista de enteros (codificando una secuencia de palabras). Las variables `train_labels` y `test_labels` son listas binarias, donde 0 indica que la opinión asociada es \"negativa\" y 1 que es \"positiva\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes observar que los índices de las palabras almacenadas no superan el 10,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no es necesario para la labor de aprendizaje que vamos a llevar a cabo, podemos dar funciones que reconstruyen las opiniones a partir de las secuencias de índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index is a dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "# We reverse it, mapping integer indices to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# We decode the review; note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos\n",
    "\n",
    "Como las redes neuronales no admiten listas de enteros como entrada, porque son de longitud variables, hemos de preprocesar nuestros datos para poder trabajar con ellas. Tenemos dos opciones:\n",
    "\n",
    "* Completar las listas más cortas para que todas tengan la misma longitud, y entonces convertirlas en tensores que alimentarán la capa de entrada de la red.\n",
    "* Codificar en One-hot las listas para convertirlas en vectores de 0s y 1s. Como tenemos un máximo de 10,000 palabras en nuestro vocabulario, cada opinión se convertirá en una lista binaria de 10,000 posiciones indicando qué palabras aparecen en la opinión. En este caso, la primera capa (densa) de nuestra red se conectará con vectores de longitud 10,000.\n",
    "\n",
    "Optaremos por esta segunda opción, que con el tiempo veremos que tiene más ventajas que la primera. \n",
    "\n",
    "El código que permite hacer esta conversión es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el nuevo aspecto que tendría una opinión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También tendríamos que hacer lo mismo con las etiquetas, pero en este caso es directo porque ya son vectores binarios, así que basta convertirlos en numéricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo la Red\n",
    "\n",
    "Como hemos visto, los datos de entrada son vectores, y las etiquetas son escalares (1s y 0s), así que estamos ante la configuración más sencilla de las posibles. Un tipo de red que funciona bien con este tipo de problemas es una simple pila de capas densas con activaciones `relu`: `layers.Dense(16, activation='relu')`.\n",
    "\n",
    "Los argumentos más habituales que tendrás que usar serán el número de unidades de la capa, y el tipo de activación que usarán. En el caso anterior usamos 16 neuronas, lo que significa que esta capa usará 16 dimensiones para intentar estructuras los patrones que encuentre en sus datos de entrada según la función objetivo (loss) que deba optimizar. Puedes interpretar intuitivamente que la dimensión de la capa representa cuánta libertad permites a la red para aprender representaciones internas. Tener más unidades permite a la red aprender representaciones más complejas, pero también aumenta la carga computacional y facilita la memorización de patrones en los datos de entrenamiento (que quizás no sean relevantes para el problema).\n",
    "\n",
    "En realidad, hay dos decisiones claves respecto a la arquitectura cuando se trabaja con capas densas:\n",
    "\n",
    "* Cuántas capas usar.\n",
    "* Cuántas unidades colocar en cada capa.\n",
    "\n",
    "En los ejemplos sucesivos iremos aprendiendo cómo tomar estas decisiones, pero en este nivel confiaremos en la arquitectura que vamos a elegir para ver cómo funciona con nuestro problema: dos capas intermedias de 16 neuronas cada una, y una tercera capa que tendrá una única salida escalar (que representará la predicción del modelo). Las capas inermedias usarás `relu` como función de activación, y la capa final usará una sigmoide (que tiene una salida en `[0, 1]`).\n",
    "\n",
    "La implementación en Keras, similar a la que ya hicimos para MNIST, es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Siguiendo el mismo patrón que vimos en el ejemplo anterior, necesitamos elegir la función de pérdida (que será minimizada) y el método de optimización (que conseguirá esa minimización). \n",
    "\n",
    "Como estamos ante un problema de clasificación binaria y la salida de nuestra red es una probabilidad (proporcionada por la sigmoide), usaremos `binary_crossentropy` como función de pérdida. No es la única opción viable, podríamos haber elegido, por ejemplo, `mean_squared_error`, pero en este caso `binary_crossentropy` es una mejor opción por estar trabajando con probabilidades. La entropía cruzada proviene del campo de Teoría de la Información que mide la distancia entre distribuciones de probabilidad (en este caso, la distribución calculada por el predictor y la que representa la distribución real proveniente de los datos de entrenamiento).\n",
    "\n",
    "Como optimizador usaremos `rmsprop`, que suele ser una buena elección en casi todos los casos. Para monitorear la evolución del aprendizaje usaremos una sola métrica, accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que en este caso estamos pasando todos los datos como cadenas, que vienen predefinidas en Keras. Pero si quieres ajustar con más flexibilidad cada una de ellas y configurar los parámetros de los que depende, puedes pasarle funciones, ya sean las que trae Keras o generadas por ti. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O incluso de forma más detallada como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validando el modelo\n",
    "\n",
    "Con el fin de monitorizar la métrica (accuracy) mientras se produce el entrenamiento necesitamos tener más datos que el modelo no use durante el proceso. Crearemos para ello un *conjunto de validación* dejando aparte 10,000 muestras del conjunto original. Así pues la situación queda como:\n",
    "\n",
    "* Conjunto de entrenamiento, con el que intentaremos optimizar los pesos de la red para que minimice la función de pérdida. En este proceso se usa un modelo que depende de ciertos parámetros que quizás deban ser ajustados para conseguir mejorar el rendimiento.\n",
    "\n",
    "* Conjunto de validación, con el que se medirá cómo de bueno es el modelo concreto que estamos entrenando (con unos parámetros fijos). Es algo así como un conjunto de test temporal.\n",
    "\n",
    "* Conjunto de test, que no se ha usado en ningún momento de las iteraciones anteriores y que permite medir de forma objetiva la bondad del modelo finalmente obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, entrenaremos el modelo con 20 epochs (20 iteraciones sobre el conjunto completo de entrenamiento, ni validación, ni test), en mini-batches de 512 muestras. Monitorearemos loss y accuracy sobre las 10,000 muestras que dejamos en el conjunto de validación. Para ello, usamos el argumento `validation_data` de la función `fit`, que no usamos en el ejemplo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que la llamada a `fit()` devuelve un objeto `history`, que tiene la siguiente estructura:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiene 4 entradas, una por cada métrica que está siendo monitorizada durante el entrenamiento y durante la validación. Podemos usar Matplotlib para representar las pérdidas/precisión de entrenamiento y validación simultáneamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['binary_accuracy']\n",
    "val_acc_values = history_dict['val_binary_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Estas gráficas muestras que la pérdida de entrenamiento decrece en cada epoch, y que el accuracy en entrenamiento crece, algo que indica que el procedimiento de optimización está funcionando adecuadamente (sobre todo respecto a la función de pérdida). Pero en eset caso observamos que no ocurre lo mismo con la validación, que empiezan a empeorar a partir de la epoch 4. Estamos ante un caso de *overfitting*: tras unos pocos pasos el sistema sobre-optimiza en los datos de entrenamiento, y aprende una representación que es específica a estos datos y que no generaliza a otros.\n",
    "\n",
    "En este caso, para prevenir el overfitting podríamos parar el entrenamiento tras las 3 primeras iteraciones. Más adelante veremos algunas otras técnicas para mitigar este efecto, pero por ahora nos contentaremos con este procedimiento.\n",
    "\n",
    "Vamos a entrenar una nueva red desde el principio pero solo durante 4 epochs y después evaluaremos el modelo sobre los datos de test (observa que estos datos no los hemos usado en ningún momento hasta ahora):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la aproximación tan simple que hemos hecho consigue un accuracy del 88% (el estado del arte actual para este problema ronda del 95%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando Predicciones sobre datos nuevos\n",
    "\n",
    "Tras haber entrenado la red el paso natural es usar el modelo para algo práctico. Puedes generar nuevas predicciones sobre opiniones para analizar si son positivas o no usando  el método `predict` asociado al modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verás que el modelo es muy determinante en algunos casos (alcanzando valores como 0.99 o 0.01) pero no tanto en otros (como 0.46)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajo Propuesto\n",
    "\n",
    "* En los modelos anteriores hemos usado 2 capas ocultas... comprueba el efecto de ampliar o reducir este número sobre la accuracy de validación y de test.\n",
    "* Cambia el número de unidades en las capas ocultas (8, 32, 64,...).\n",
    "* Mira qué ocurre al usar `mse` como función de pérdida, en vez de `binary_crossentropy`.\n",
    "* Mira qué ocurre al usar la activación `tanh` en vez de `relu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Algunas conclusiones que podemos ir ya apuntando:\n",
    "\n",
    "* El trabajo de preprocesamiento sobre los datos es una etapa esencial para que puedas alimentar a las redes (no olvides que se alimentan de tensores)\n",
    "* Las secuencias de palabras se pueden codificar como vectores binarios, aunque hay otras opciones.\n",
    "* Las pilas de capas densas con activaciones `relu` pueden resolver una amplia variedad de problemas... así que no as olvides.\n",
    "* En los problemas de clasificación binaria la red debería acabar en una capa densa con una sola neurona que haga uso de la activación `sigmoid`. De esta forma, la salida será un únic escalar en `[0,1]` que se puede interpretar como una probabilidad.\n",
    "* En este caso (salida sigmoide en un problema de clasificación binario) una función de pérdida adecuada es `binary_crossentropy`.\n",
    "* El optimizador `rmsprop` es normalmente una buena opción... y para casi todos los problemas.\n",
    "* Cuidado con el *overfitting*. Asegúrate de monitorear la evolución del entrenamiento (acuérdate de preparar un conjunto de validación adicional)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
